apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: polarwarp-gcps
spec:
  entrypoint: execute
  templates:
    - name: execute
      inputs:
        parameters:
          - name: lon_min
          - name: lat_min
          - name: lon_max
          - name: lat_max
          - name: date
          - name: num_gcps
      steps:
        - - name: fetch-predictions
            template: fetch-predictions
            arguments:
              parameters:
                - name: datestripped
                  value: '{{= replace(inputs.parameters.date, "-", "") }}'
          - name: fetch-s1
            template: fetch-s1
            arguments:
              parameters:
                - name: lon_min
                  value: "{{inputs.parameters.lon_min}}"
                - name: lat_min
                  value: "{{inputs.parameters.lat_min}}"
                - name: lon_max
                  value: "{{inputs.parameters.lon_max}}"
                - name: lat_max
                  value: "{{inputs.parameters.lat_max}}"
                - name: date
                  value: "{{inputs.parameters.date}}"
        - - name: polarwarp
            template: polarwarp
            arguments:
              parameters:
                - name: lon_min
                  value: "{{inputs.parameters.lon_min}}"
                - name: lat_min
                  value: "{{inputs.parameters.lat_min}}"
                - name: lon_max
                  value: "{{inputs.parameters.lon_max}}"
                - name: lat_max
                  value: "{{inputs.parameters.lat_max}}"
                - name: date
                  value: "{{inputs.parameters.date}}"
                - name: num_gcps
                  value: "{{inputs.parameters.num_gcps}}"
              artifacts:
                - name: image_data
                  from: "{{steps.fetch-s1.outputs.artifacts.s1data}}"
                - name: model_data
                  from: "{{steps.fetch-predictions.outputs.artifacts.predictions}}"

    - name: fetch-predictions
      inputs:
        parameters:
          - name: datestripped
      outputs:
        artifacts:
          - name: predictions
            path: /output
            archive:
              none: {}
      script:
        image: copernicusmarine/copernicusmarine:2.1.3
        command: ["/bin/bash", "-c"]
        args:
          - >
            copernicusmarine login --username xxx --password xxx &&
            mkdir -p /output &&
            copernicusmarine get -nd -o /output
            --dataset-id cmems_mod_arc_phy_anfc_nextsim_hm
            --regex '{{inputs.parameters.datestripped}}_hr'
            --force-download
        env:
          - name: COPERNICUS_USERNAME
            value: xxx
          - name: COPERNICUS_PASSWORD
            value: xxx

    - name: fetch-s1
      inputs:
        parameters:
          - name: lon_min
          - name: lat_min
          - name: lon_max
          - name: lat_max
          - name: date
      outputs:
        artifacts:
          - name: s1data
            path: /output
            archive:
              none: {}
      script:
        image: santilland/destinelab-python3.9-slim:0.0.2
        command: ["python", "-c"]
        args:
          - |-
            import requests 
            from pyproj import Transformer
            import os 
            import destinelab as deauth 
            import zipfile 

            DESP_USERNAME = 'xxx' 
            DESP_PASSWORD = 'xxx' 
            auth = deauth.AuthHandler(DESP_USERNAME, DESP_PASSWORD) 
            access_token = auth.get_token() 

            HDA_API_URL = 'https://hda.data.destination-earth.eu' 
            SEARCH_URL = HDA_API_URL + '/stac/search' 

            transformer = Transformer.from_crs("EPSG:3413", "EPSG:4326", always_xy=True)

            min_lon, min_lat = transformer.transform({{workflow.parameters.lon_min}}, {{workflow.parameters.lat_min}})
            max_lon, max_lat = transformer.transform({{workflow.parameters.lon_max}}, {{workflow.parameters.lat_max}})

            bb = (min_lon, min_lat, max_lon, max_lat)
            SEARCH_QUERY_STRING = (
              '?collections=EO.ESA.DAT.SENTINEL-1.L1_GRD'
              '&datetime={{workflow.parameters.date}}T00:00:00Z/{{workflow.parameters.date}}T23:59:59Z'
              '&bbox=%s,%s,%s,%s&limit=1' % (bb[0], bb[1], bb[2], bb[3])
            ) 

            print(SEARCH_URL + SEARCH_QUERY_STRING) 

            resp = requests.get(SEARCH_URL + SEARCH_QUERY_STRING, headers={'Authorization': f'Bearer {access_token}'}) 
            data = resp.json() 

            if not data['features']:
                raise Exception('No product found for query')

            item = data['features'][-1] 
            item_url = item['assets']['downloadLink']['href'] 
            print(item_url) 

            dataresp = requests.get(item_url, headers={'Authorization': f'Bearer {access_token}'}) 
            with open('download.zip', 'wb') as f: 
                f.write(dataresp.content) 

            os.makedirs('/output', exist_ok=True) 
            with zipfile.ZipFile('download.zip','r') as zip_ref: 
                zip_ref.extractall('/output')

    - name: polarwarp
      inputs:
        parameters:
          - name: lon_min
          - name: lat_min
          - name: lon_max
          - name: lat_max
          - name: date
          - name: num_gcps
        artifacts:
          - name: image_data
            path: /image_data/
            mode: 511
          - name: model_data
            path: /model_data/
            mode: 511
      outputs:
        artifacts:
          - name: results
            path: /out_dir
            archive:
              none: {}
      script:
        image: swr.eu-nl.otc.t-systems.com/gtif/polarwarp:0.0.2
        command: ["/bin/sh"]
        securityContext:
          runAsUser: 0
        source: |
          mkdir -p /out_dir
          find /image_data/ -maxdepth 1 -mindepth 1 -type d | while read folder; do
              folder_name=$(basename "$folder")
              echo "Processing file: ${folder_name}"
              gdalwarp -t_srs EPSG:3413 -of GTiff -co COMPRESS=DEFLATE \
                -te {{workflow.parameters.lon_min}} {{workflow.parameters.lat_min}} {{workflow.parameters.lon_max}} {{workflow.parameters.lat_max}} \
                "/image_data/${folder_name}" "/image_data/${folder_name}.tif"
              python3 priima/image_forecast.py \
                --image "/image_data/${folder_name}.tif" \
                --output-resolution 100 \
                --forecast-duration 6 \
                --data-source NEXTSIM \
                --num-gcps {{workflow.parameters.num_gcps}}
          done

          # Find the folder dynamically we assume one result
          DETECTED_FOLDER=$(ls -d /out_dir/*/ | head -n 1) 
          echo "Detected folder: $DETECTED_FOLDER"

          # Move it to a fixed location for artifact capture
          mv "$DETECTED_FOLDER" /out_dir/scene

          # Delete all non-TIF files inside the folder
          find /out_dir/scene -type f ! -name "*.tif" -delete

          # Make sure all files have the same extent
          for tiffile in $(find /out_dir/scene -type f -name "*.tif"); do
              echo "Recutting file: ${tiffile}"
              gdalwarp -t_srs EPSG:3413 -of COG -co COMPRESS=DEFLATE \
                -te {{workflow.parameters.lon_min}} {{workflow.parameters.lat_min}} {{workflow.parameters.lon_max}} {{workflow.parameters.lat_max}} \
                -dstnodata 0 -overwrite "${tiffile}" "/out_dir/scene/tmp.tif"
              mv "/out_dir/scene/tmp.tif" "${tiffile}"
          done
